{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation for Topic Labeling using Top2Vec (Gutenberg version)\n",
    "\n",
    "by Andreas SÃ¼nder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model_id = 'jinaai/jina-embeddings-v2-base-de'\n",
    "books_path = ''\n",
    "limit = 2048\n",
    "stop_words = 'english'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModel.from_pretrained(emb_model_id, trust_remote_code=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(emb_model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert documents to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset\n",
    "import re\n",
    "\n",
    "def process_texts():\n",
    "  for filename in os.listdir(books_path):\n",
    "    if re.fullmatch(r'book_\\d*.txt', filename) is None:\n",
    "      continue\n",
    "\n",
    "    with open(os.path.join(books_path, filename), 'r') as file:\n",
    "      try:\n",
    "        text = file.read()\n",
    "      except:\n",
    "        continue\n",
    "      \n",
    "      index_start = text.find(\"** START\")\n",
    "      index_start = text.find(\"\\n\", index_start) + 1\n",
    "      index_end = text.find(\"** END\")\n",
    "      text = text[index_start:index_end]\n",
    "      \n",
    "      batch_dict = tokenizer(text, return_tensors=\"pt\")\n",
    "      tokens = batch_dict[\"input_ids\"].squeeze()\n",
    "      token_parts = [tokens[i : i + limit] for i in range(0, len(tokens), limit)]\n",
    "      parts = []\n",
    "\n",
    "      for part in token_parts:\n",
    "        parts.append(tokenizer.decode(part))\n",
    "\n",
    "      if parts:\n",
    "        for part in parts:\n",
    "          with torch.no_grad():\n",
    "            embedding = model.encode(part, show_progress_bar=False)\n",
    "            yield {\"text\": part, \"embedding\": embedding}\n",
    "\n",
    "dataset = Dataset.from_generator(process_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "umap = UMAP(n_neighbors=15, n_components=5, metric='cosine', verbose=True)\n",
    "umap_embeddings = umap.fit_transform(dataset['embedding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdb = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom')\n",
    "cluster = hdb.fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "umap = UMAP(n_neighbors=15, n_components=2, metric='cosine', verbose=True)\n",
    "umap_data = umap.fit_transform(dataset['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "result['labels'] = cluster.labels_\n",
    "outliers = result.loc[result.labels == -1, :]\n",
    "clustered = result.loc[result.labels != -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(result)\n",
    "print(f'Number of clusters: {len(set(cluster.labels_))}')\n",
    "print(f'Ratio clustered/outliers: {len(clustered)/total:.1f}/{len(outliers)/total:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.5)\n",
    "ax.scatter(clustered.x, clustered.y, c=clustered.labels, s=1, cmap='turbo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save the clustered dataset\n",
    "dataset_clustered = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "dataset_clustered['title'] = dataset['title']\n",
    "dataset_clustered['topic'] = cluster.labels_\n",
    "dataset_clustered['topic'] = dataset_clustered['topic'].map('Topic {}'.format)\n",
    "dataset_clustered.to_csv('mn-ds_clustered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = dataset.remove_columns(['embedding']).to_pandas()\n",
    "\n",
    "docs_df = pd.DataFrame(df, columns=['text'])\n",
    "docs_df['topic'] = cluster.labels_\n",
    "docs_df = docs_df[docs_df.topic != -1]\n",
    "docs_per_topic = docs_df.groupby(['topic'], as_index=False).agg({'text': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if stop_words == 'german':\n",
    "  stop_words = open('german_stopwords.txt', 'r').read().splitlines()\n",
    "\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 2)):\n",
    "  count = CountVectorizer(ngram_range=ngram_range, stop_words=stop_words).fit(documents)\n",
    "  t = count.transform(documents).astype(np.uint8).toarray()\n",
    "  w = t.sum(axis=1)\n",
    "  tf = np.divide(t.T, w)\n",
    "  sum_t = t.sum(axis=0)\n",
    "  idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "  tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "  return tf_idf, count\n",
    "  \n",
    "tf_idf, count = c_tf_idf(docs_per_topic.text.values, m=len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "  words = count.get_feature_names_out()\n",
    "  labels = list(docs_per_topic.topic)\n",
    "  tf_idf_transposed = tf_idf.T\n",
    "  indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "  top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "  return top_n_words\n",
    "\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in range(len(top_n_words)):\n",
    "  print(', '.join([word[0] for word in top_n_words[topic][:10]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
