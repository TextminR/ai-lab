{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Training on Mistral 7B for NER\n",
    "\n",
    "by Benjamin Kissinger & Andreas SÃ¼nder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages (only needed once)\n",
    "\n",
    "```bash\n",
    "%pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Open up a terminal and run the following commands:\n",
    "\n",
    "```bash\n",
    "huggingface-cli login\n",
    "wandb login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "DATA_DIR      = 'data'\n",
    "TRAINING_DATA = 'train.jsonl'\n",
    "VAL_DATA      = 'val.jsonl'\n",
    "\n",
    "train_dataset = load_dataset('json', data_files=os.path.join(DATA_DIR, TRAINING_DATA), split='train')\n",
    "eval_dataset = load_dataset('json', data_files=os.path.join(DATA_DIR, VAL_DATA), split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '### Question: {prompt}\\n ### Answer: {response}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer,\n",
    "                          BitsAndBytesConfig)\n",
    "\n",
    "base_model_id = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "  load_in_4bit=True,\n",
    "  bnb_4bit_use_double_quant=True,\n",
    "  bnb_4bit_quant_type='nf4',\n",
    "  bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, torch_dtype=torch.float16, device_map='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  base_model_id,\n",
    "  padding_side='left',\n",
    "  add_eos_token=True,\n",
    "  add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(prompt_template.format(**prompt))\n",
    "    \n",
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_test_dataset):\n",
    "  lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "  lengths += [len(x['input_ids']) for x in tokenized_test_dataset]\n",
    "  \n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "  plt.xlabel('Length of input_ids')\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.title('Distribution of Lengths of input_ids')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 55 # This was an appropriate max length for the dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "  result = tokenizer(\n",
    "    prompt_template.format(**prompt),\n",
    "    truncation=True,\n",
    "    max_length=max_length,\n",
    "    padding='max_length',\n",
    "  )\n",
    "  result['labels'] = result['input_ids'].copy()\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "  trainable_params = 0\n",
    "  all_param = 0\n",
    "  for _, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if param.requires_grad:\n",
    "      trainable_params += param.numel()\n",
    "\n",
    "  print(f'trainable params: {trainable_params} || all params: {all_param} || trainable: {100 * trainable_params / all_param: .2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "  r=32,\n",
    "  lora_alpha=64,\n",
    "  target_modules=[\n",
    "    'q_proj',\n",
    "    'k_proj',\n",
    "    'v_proj',\n",
    "    'o_proj',\n",
    "    'gate_proj',\n",
    "    'up_proj',\n",
    "    'down_proj',\n",
    "    'lm_head',\n",
    "  ],\n",
    "  bias='none',\n",
    "  lora_dropout=0.05,\n",
    "  task_type='CAUSAL_LM',\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'ner_qlora_mistral'\n",
    "%env WANDB_PROJECT=$project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from transformers import (DataCollatorForLanguageModeling,\n",
    "                          EarlyStoppingCallback, Trainer, TrainingArguments)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=project_name,\n",
    "        warmup_steps=1,\n",
    "        per_device_train_batch_size=3,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=1000,\n",
    "        learning_rate=2e-5,\n",
    "        bf16=True,\n",
    "        optim='paged_adamw_8bit',\n",
    "        logging_steps=200,              # When to start reporting loss\n",
    "        logging_dir='./logs',        # Directory for storing logs\n",
    "        save_strategy='steps',       # Save the model checkpoint every logging step\n",
    "        save_steps=200,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy='steps', # Evaluate the model every logging step\n",
    "        eval_steps=200,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "        metric_for_best_model='loss',  # Use loss to determine the best model\n",
    "        greater_is_better=False,       # Lower loss indicates a better model\n",
    "        report_to='wandb',           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f'{project_name}-{datetime.now().strftime(\"%Y-%m-%d-%H-%M\")}'         # Name of the W&B run (optional),\n",
    "        \n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Stop after 3 evaluations without improvement\n",
    ")\n",
    "\n",
    "model.config.use_cache = True  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub('textminr/mistral-7b-4bit-ner')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
