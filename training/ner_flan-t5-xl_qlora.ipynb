{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Google's FLAN-T5 model for NER\n",
    "\n",
    "by Benjamin Kissinger & Andreas SÃ¼nder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages (only once)\n",
    "\n",
    "```bash\n",
    "%pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Open up a terminal and run the following commands:\n",
    "\n",
    "```bash\n",
    "huggingface-cli login\n",
    "wandb login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 16194\n",
      "Test dataset size: 4049\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('textminr/ner-test2', 'base', split='train')\n",
    "eval_dataset = load_dataset('textminr/ner-test2', 'base', split='validation')\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = 'Input sentence: {input} --- Output: {output}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4017334bf30442ab77c5afcf75f59d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = 'google/flan-t5-xl'\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "  # load_in_4bit=True,\n",
    "  load_in_8bit=True,\n",
    "  # bnb_4bit_use_double_quant=True,\n",
    "  # bnb_4bit_quant_type='nf4',\n",
    "  # bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_model_id, quantization_config=bnb_config, torch_dtype=torch.bfloat16, device_map='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 55\n",
    "max_target_length = 30\n",
    "\n",
    "def preprocess_data(sample: str, padding: str = 'max_length'):\n",
    "  model_inputs = tokenizer(\n",
    "    # sample['words'],\n",
    "    sample['prompt'],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_source_length\n",
    "  )\n",
    "\n",
    "  labels = tokenizer(\n",
    "    # text_target=sample['topic_label'],\n",
    "    text_target=sample['response'],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_target_length\n",
    "  )\n",
    "\n",
    "  if padding == 'max_length':\n",
    "    labels['input_ids'] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels['input_ids']\n",
    "    ]\n",
    "\n",
    "  model_inputs['labels'] = labels['input_ids']\n",
    "  return model_inputs\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 35,936,256 || all params: 2,885,693,440 || trainable%: 1.2453247979106195\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "  r=16,\n",
    "  lora_alpha=32,\n",
    "  target_modules=['q', 'k', 'v', 'o', 'wi_0', 'wi_1', 'wo', 'lm_head'],\n",
    "  bias='none',\n",
    "  lora_dropout=0.05,\n",
    "  task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "label_pad_token_id = -100\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "  tokenizer,\n",
    "  model=model,\n",
    "  label_pad_token_id=label_pad_token_id,\n",
    "  pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=ner_flan-t5-xl_qlora_8bit\n"
     ]
    }
   ],
   "source": [
    "project_name = 'ner_flan-t5-xl_qlora_8bit'\n",
    "%env WANDB_PROJECT=$project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33masuender\u001b[0m (\u001b[33mtextminr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/asuender/Documents/tgm/textminr/ai-lab/training/wandb/run-20231203_141813-5q17dd7u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/textminr/ner_flan-t5-xl_qlora_8bit/runs/5q17dd7u' target=\"_blank\">ner_flan-t5-xl_qlora_8bit-2023-12-03-14-18</a></strong> to <a href='https://wandb.ai/textminr/ner_flan-t5-xl_qlora_8bit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/textminr/ner_flan-t5-xl_qlora_8bit' target=\"_blank\">https://wandb.ai/textminr/ner_flan-t5-xl_qlora_8bit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/textminr/ner_flan-t5-xl_qlora_8bit/runs/5q17dd7u' target=\"_blank\">https://wandb.ai/textminr/ner_flan-t5-xl_qlora_8bit/runs/5q17dd7u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735aa1f1fd0f4c07826e08113209d18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/asuender/miniconda3/envs/ds/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/asuender/miniconda3/envs/ds/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/asuender/miniconda3/envs/ds/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6287, 'learning_rate': 1.9e-05, 'epoch': 0.02}\n",
      "{'loss': 1.734, 'learning_rate': 1.8e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8567, 'learning_rate': 1.7e-05, 'epoch': 0.07}\n",
      "{'loss': 0.3221, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda6e84eec3548cf9063f058eb0dd52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13623377680778503, 'eval_runtime': 150.5732, 'eval_samples_per_second': 26.891, 'eval_steps_per_second': 3.367, 'epoch': 0.1}\n",
      "{'loss': 0.1657, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0745, 'learning_rate': 1.4e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0495, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0359, 'learning_rate': 1.2e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41372abde00e4210b823067f7ce41f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.008787570521235466, 'eval_runtime': 154.3353, 'eval_samples_per_second': 26.235, 'eval_steps_per_second': 3.285, 'epoch': 0.2}\n",
      "{'loss': 0.0337, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0224, 'learning_rate': 1e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0203, 'learning_rate': 9e-06, 'epoch': 0.27}\n",
      "{'loss': 0.0183, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.3}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d9f003726942e08c4626c993de353f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0049870978109538555, 'eval_runtime': 150.2585, 'eval_samples_per_second': 26.947, 'eval_steps_per_second': 3.374, 'epoch': 0.3}\n",
      "{'loss': 0.019, 'learning_rate': 7e-06, 'epoch': 0.32}\n",
      "{'loss': 0.0203, 'learning_rate': 6e-06, 'epoch': 0.35}\n",
      "{'loss': 0.0188, 'learning_rate': 5e-06, 'epoch': 0.37}\n",
      "{'loss': 0.0154, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0ffbdb673048acb540eff8fc29a40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0042031980119645596, 'eval_runtime': 151.4888, 'eval_samples_per_second': 26.728, 'eval_steps_per_second': 3.347, 'epoch': 0.4}\n",
      "{'loss': 0.0136, 'learning_rate': 3e-06, 'epoch': 0.42}\n",
      "{'loss': 0.0158, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.44}\n",
      "{'loss': 0.0194, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.47}\n",
      "{'loss': 0.0164, 'learning_rate': 0.0, 'epoch': 0.49}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3683b631efbc42abb427e61d645c5e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004072191659361124, 'eval_runtime': 149.076, 'eval_samples_per_second': 27.161, 'eval_steps_per_second': 3.401, 'epoch': 0.49}\n",
      "{'train_runtime': 2314.6012, 'train_samples_per_second': 3.456, 'train_steps_per_second': 0.432, 'train_loss': 0.30503430783748625, 'epoch': 0.49}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.30503430783748625, metrics={'train_runtime': 2314.6012, 'train_samples_per_second': 3.456, 'train_steps_per_second': 0.432, 'train_loss': 0.30503430783748625, 'epoch': 0.49})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datetime import datetime\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "  output_dir=project_name,\n",
    "  auto_find_batch_size=True,\n",
    "  # per_device_train_batch_size=1,\n",
    "  # per_device_eval_batch_size=8,\n",
    "  predict_with_generate=True,\n",
    "  optim='adafactor',\n",
    "  bf16=True,\n",
    "  # num_train_epochs=0.5,\n",
    "  max_steps=1000,\n",
    "  learning_rate=2e-5,\n",
    "  logging_steps=50,\n",
    "  logging_dir='./logs',\n",
    "  save_strategy='steps',\n",
    "  save_steps=1000,\n",
    "  do_eval=True,\n",
    "  evaluation_strategy='steps',\n",
    "  eval_steps=200,\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model='loss',\n",
    "  greater_is_better=False,\n",
    "  report_to='wandb',\n",
    "  run_name=f'{project_name}-{datetime.now().strftime(\"%Y-%m-%d-%H-%M\")}'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=tokenized_train_dataset,\n",
    "  eval_dataset=tokenized_eval_dataset,\n",
    "  data_collator=data_collator,\n",
    "  tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02642275a9e47a6a2329a4c67c724d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/144M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/textminr/ner-flan-t5-xl/commit/aa9c46f2cfc1bc5062bfa90774d60d058a9e02b7', commit_message='Upload model', commit_description='', oid='aa9c46f2cfc1bc5062bfa90774d60d058a9e02b7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('textminr/ner-flan-t5-xl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
