{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT models for NER\n",
    "\n",
    "by Benjamin Kissinger & Andreas SÃ¼nder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages (only once)\n",
    "\n",
    "```bash\n",
    "%pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Open up a terminal and run the following commands:\n",
    "\n",
    "```bash\n",
    "huggingface-cli login\n",
    "wandb login\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'distilbert-base-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('textminr/ner_tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['O', 'AUTHOR', 'DATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(row):\n",
    "  tokenized_inputs = tokenizer(row['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "  labels = []\n",
    "  for i, label in enumerate(row[f'ner_ids']):\n",
    "    word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "      if word_idx is None:\n",
    "        label_ids.append(-100)\n",
    "      elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "        label_ids.append(label[word_idx])\n",
    "      else:\n",
    "        label_ids.append(-100)\n",
    "      previous_word_idx = word_idx\n",
    "    labels.append(label_ids)\n",
    "\n",
    "  tokenized_inputs['labels'] = labels\n",
    "  return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "seqeval = evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "  predictions, labels = p\n",
    "  predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "  true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "  ]\n",
    "  true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "  ]\n",
    "\n",
    "  results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "  return {\n",
    "    'precision': results['overall_precision'],\n",
    "    'recall': results['overall_recall'],\n",
    "    'f1': results['overall_f1'],\n",
    "    'accuracy': results['overall_accuracy'],\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tag= {\n",
    "  0: 'O',\n",
    "  1: 'AUTHOR',\n",
    "  2: 'DATE',\n",
    "}\n",
    "\n",
    "tag2id = {v: k for k, v in id2tag.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "  model_id,\n",
    "  num_labels=len(label_list),\n",
    "  id2label=id2tag,\n",
    "  label2id=tag2id,\n",
    "  # device_map='cuda:0',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "project_name = 'ner-distilbert-english'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=f'models/{project_name}',\n",
    "  fp16=False,\n",
    "  learning_rate=2e-5,\n",
    "  auto_find_batch_size=True,\n",
    "  num_train_epochs=1,\n",
    "  evaluation_strategy='steps',\n",
    "  eval_steps=200,\n",
    "  save_strategy='steps',\n",
    "  save_steps=1000,\n",
    "  load_best_model_at_end=True,\n",
    ")\n",
    "  \n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=tokenized_datasets['train'],\n",
    "  eval_dataset=tokenized_datasets['validation'],\n",
    "  tokenizer=tokenizer,\n",
    "  data_collator=data_collator,\n",
    "  compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33masuender\u001b[0m (\u001b[33mtextminr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/asuender/Documents/tgm/textminr/ai-lab/training/wandb/run-20231205_142422-ejht6gdx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/textminr/huggingface/runs/ejht6gdx' target=\"_blank\">elated-fog-4</a></strong> to <a href='https://wandb.ai/textminr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/textminr/huggingface' target=\"_blank\">https://wandb.ai/textminr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/textminr/huggingface/runs/ejht6gdx' target=\"_blank\">https://wandb.ai/textminr/huggingface/runs/ejht6gdx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1d3132f98d47059888d2959a25956c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534a7a70b6c3453aac267293ce356c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asuender/miniconda3/envs/ds/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUTHOR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/asuender/miniconda3/envs/ds/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DATE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00324088241904974, 'eval_precision': 0.9988090512107979, 'eval_recall': 0.9913317572892041, 'eval_f1': 0.9950563575242239, 'eval_accuracy': 0.9991569772065724, 'eval_runtime': 3.917, 'eval_samples_per_second': 1033.692, 'eval_steps_per_second': 129.435, 'epoch': 0.1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8bd9b1e8fb4320ae81221f15f11428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0015421499265357852, 'eval_precision': 0.9998020194020986, 'eval_recall': 0.9948778565799843, 'eval_f1': 0.9973338599782758, 'eval_accuracy': 0.9995546294676232, 'eval_runtime': 3.883, 'eval_samples_per_second': 1042.755, 'eval_steps_per_second': 130.57, 'epoch': 0.2}\n",
      "{'loss': 0.0218, 'learning_rate': 1.506172839506173e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0e40a8a35744b2a5210e560e36ce07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.001125755487009883, 'eval_precision': 0.9990157480314961, 'eval_recall': 0.9998029944838456, 'eval_f1': 0.999409216226861, 'eval_accuracy': 0.9998886573669058, 'eval_runtime': 3.8631, 'eval_samples_per_second': 1048.12, 'eval_steps_per_second': 131.241, 'epoch': 0.3}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e70f7657d54f1fa40ec297dfb83357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0005293559515848756, 'eval_precision': 0.9998029944838456, 'eval_recall': 0.9998029944838456, 'eval_f1': 0.9998029944838456, 'eval_accuracy': 0.9999522817286739, 'eval_runtime': 3.8978, 'eval_samples_per_second': 1038.801, 'eval_steps_per_second': 130.075, 'epoch': 0.4}\n",
      "{'loss': 0.001, 'learning_rate': 1.0123456790123458e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d7e26268744a06aeb75fd8437c0a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00031827494967728853, 'eval_precision': 0.9996059889676912, 'eval_recall': 0.9996059889676912, 'eval_f1': 0.9996059889676912, 'eval_accuracy': 0.9999363756382319, 'eval_runtime': 3.9145, 'eval_samples_per_second': 1034.371, 'eval_steps_per_second': 129.52, 'epoch': 0.49}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322fc4af180f47f4b8b67881633939c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asuender/miniconda3/envs/ds/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUTHOR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/asuender/miniconda3/envs/ds/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DATE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.000282159773632884, 'eval_precision': 0.9996060665747488, 'eval_recall': 0.9998029944838456, 'eval_f1': 0.9997045208312814, 'eval_accuracy': 0.9999522817286739, 'eval_runtime': 4.0818, 'eval_samples_per_second': 991.96, 'eval_steps_per_second': 124.209, 'epoch': 0.59}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7aae35801244358ab6c8dde97bcb7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00027990678790956736, 'eval_precision': 0.9998030332873744, 'eval_recall': 1.0, 'eval_f1': 0.9999015069437605, 'eval_accuracy': 0.9999681878191159, 'eval_runtime': 3.8802, 'eval_samples_per_second': 1043.505, 'eval_steps_per_second': 130.664, 'epoch': 0.69}\n",
      "{'loss': 0.0003, 'learning_rate': 5.185185185185185e-06, 'epoch': 0.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ed051dd6614ea7b980ed68a477d087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0002756484318524599, 'eval_precision': 0.9998030332873744, 'eval_recall': 1.0, 'eval_f1': 0.9999015069437605, 'eval_accuracy': 0.9999681878191159, 'eval_runtime': 4.027, 'eval_samples_per_second': 1005.466, 'eval_steps_per_second': 125.9, 'epoch': 0.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86501cd4daf4fc0bc2a80d69442630b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00027747408603318036, 'eval_precision': 0.9998030332873744, 'eval_recall': 1.0, 'eval_f1': 0.9999015069437605, 'eval_accuracy': 0.9999681878191159, 'eval_runtime': 3.9227, 'eval_samples_per_second': 1032.189, 'eval_steps_per_second': 129.247, 'epoch': 0.89}\n",
      "{'loss': 0.0003, 'learning_rate': 2.469135802469136e-07, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd59cceb49e4ec890fb417e1437d20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00026903985417447984, 'eval_precision': 0.9998030332873744, 'eval_recall': 1.0, 'eval_f1': 0.9999015069437605, 'eval_accuracy': 0.9999681878191159, 'eval_runtime': 4.0134, 'eval_samples_per_second': 1008.871, 'eval_steps_per_second': 126.327, 'epoch': 0.99}\n",
      "{'train_runtime': 117.641, 'train_samples_per_second': 137.656, 'train_steps_per_second': 17.213, 'train_loss': 0.00579406047096177, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2025, training_loss=0.00579406047096177, metrics={'train_runtime': 117.641, 'train_samples_per_second': 137.656, 'train_steps_per_second': 17.213, 'train_loss': 0.00579406047096177, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'distilbert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\n",
    "  'ner',\n",
    "  model='models/ner-distilbert-english/checkpoint-2000',\n",
    "  tokenizer=model_id,\n",
    "  aggregation_strategy='simple'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"In his book 'The Whale' Herman Melville mentions the date 1851. He was born in 1819. His mother, Maria Gansevoort, died in 1832.\" \n",
    "sentence = \"In his book 'The Whale' Herman Melville mentions the date 1851. He was born in 1819 and wrote 'The Whale' in 2020. His mother, Maria Gansevoort, died in 1832.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'AUTHOR',\n",
       "  'score': 0.99893075,\n",
       "  'word': 'Herman Melville',\n",
       "  'start': 24,\n",
       "  'end': 39},\n",
       " {'entity_group': 'DATE',\n",
       "  'score': 0.9672856,\n",
       "  'word': '1851',\n",
       "  'start': 58,\n",
       "  'end': 62},\n",
       " {'entity_group': 'DATE',\n",
       "  'score': 0.96138334,\n",
       "  'word': '1819',\n",
       "  'start': 79,\n",
       "  'end': 83},\n",
       " {'entity_group': 'DATE',\n",
       "  'score': 0.999453,\n",
       "  'word': '2020',\n",
       "  'start': 109,\n",
       "  'end': 113},\n",
       " {'entity_group': 'AUTHOR',\n",
       "  'score': 0.9973533,\n",
       "  'word': 'Maria G',\n",
       "  'start': 127,\n",
       "  'end': 134},\n",
       " {'entity_group': 'DATE',\n",
       "  'score': 0.99920195,\n",
       "  'word': '1832',\n",
       "  'start': 153,\n",
       "  'end': 157}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
