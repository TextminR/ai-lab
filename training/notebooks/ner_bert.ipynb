{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT models for NER\n",
    "\n",
    "by Benjamin Kissinger & Andreas SÃ¼nder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages (only once)\n",
    "\n",
    "```bash\n",
    "%pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Open up a terminal and run the following commands:\n",
    "\n",
    "```bash\n",
    "huggingface-cli login\n",
    "wandb login\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'distilbert-base-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('textminr/ner_tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['O', 'AUTHOR', 'DATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(row):\n",
    "  tokenized_inputs = tokenizer(row['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "  labels = []\n",
    "  for i, label in enumerate(row[f'ner_ids']):\n",
    "    word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "      if word_idx is None:\n",
    "        label_ids.append(-100)\n",
    "      elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "        label_ids.append(label[word_idx])\n",
    "      else:\n",
    "        label_ids.append(-100)\n",
    "      previous_word_idx = word_idx\n",
    "    labels.append(label_ids)\n",
    "\n",
    "  tokenized_inputs['labels'] = labels\n",
    "  return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "seqeval = evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "  predictions, labels = p\n",
    "  predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "  true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "  ]\n",
    "  true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "  ]\n",
    "\n",
    "  results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "  return {\n",
    "    'precision': results['overall_precision'],\n",
    "    'recall': results['overall_recall'],\n",
    "    'f1': results['overall_f1'],\n",
    "    'accuracy': results['overall_accuracy'],\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tag= {\n",
    "  0: 'O',\n",
    "  1: 'AUTHOR',\n",
    "  2: 'DATE',\n",
    "}\n",
    "\n",
    "tag2id = {v: k for k, v in id2tag.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "  model_id,\n",
    "  num_labels=len(label_list),\n",
    "  id2label=id2tag,\n",
    "  label2id=tag2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from datetime import datetime\n",
    "\n",
    "PROJECT_NAME = 'ner_distilbert-base-cased'\n",
    "%env WANDB_PROJECT=$PROJECT_NAME\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=f'models/{PROJECT_NAME}',\n",
    "  fp16=False,\n",
    "  bf16=False,\n",
    "  learning_rate=2e-5,\n",
    "  auto_find_batch_size=True,\n",
    "  num_train_epochs=1,\n",
    "  logging_strategy='steps',\n",
    "  logging_steps=200,\n",
    "  evaluation_strategy='steps',\n",
    "  eval_steps=200,\n",
    "  report_to='wandb',\n",
    "  save_strategy='no',\n",
    "  run_name=f'{PROJECT_NAME}-{datetime.now().strftime(\"%Y-%m-%d-%H-%M\")}'\n",
    ")\n",
    "  \n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=tokenized_datasets['train'],\n",
    "  eval_dataset=tokenized_datasets['validation'],\n",
    "  tokenizer=tokenizer,\n",
    "  data_collator=data_collator,\n",
    "  compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub('textminr/ner_distil-bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'textminr/ner_distil-bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\n",
    "  'ner',\n",
    "  model=model_id,\n",
    "  tokenizer='distilbert-base-cased',\n",
    "  aggregation_strategy='simple'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"His book, written in 2013, mentions his Project and helps young people.\" \n",
    "sentence = \"Captivated by the medieval tapestry, the author Albert Einstein transcribed the wisdom of 'Ink and Parchment' by Eleanor the Wise in the year 1268\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
