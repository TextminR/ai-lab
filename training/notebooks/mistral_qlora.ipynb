{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Fine-Tuning on Mistral 7B\n",
    "\n",
    "by Benjamin Kissinger & Andreas SÃ¼nder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages (only needed once)\n",
    "\n",
    "```bash\n",
    "%pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Open up a terminal and run the following commands:\n",
    "\n",
    "```bash\n",
    "huggingface-cli login\n",
    "wandb login\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = ''\n",
    "base_model_id = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
    "prompt_template = ''\n",
    "run_name = ''\n",
    "max_input_length = -1\n",
    "hub_model_id = ''\n",
    "\n",
    "project_name = ''\n",
    "%env WANDB_PROJECT=$project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  base_model_id,\n",
    "  quantization_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "  ),\n",
    "  torch_dtype=torch.float16,\n",
    "  device_map='auto'\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  base_model_id,\n",
    "  padding_side='left',\n",
    "  add_eos_token=True,\n",
    "  add_bos_token=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample(prompt):\n",
    "  result = tokenizer(\n",
    "    prompt_template.format(**prompt),\n",
    "    padding='max_length',\n",
    "    max_length=max_input_length,\n",
    "    truncation=True,\n",
    "  )\n",
    "  result['labels'] = result['input_ids'].copy()\n",
    "  return result\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_sample, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "  trainable_params = 0\n",
    "  all_param = 0\n",
    "  for _, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if param.requires_grad:\n",
    "      trainable_params += param.numel()\n",
    "\n",
    "  print(f'trainable params: {trainable_params} || all params: {all_param} || trainable: {100 * trainable_params / all_param: .2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import TaskType\n",
    "\n",
    "config = LoraConfig(\n",
    "  r=8,\n",
    "  lora_alpha=8,\n",
    "  target_modules=[\n",
    "    'q_proj',\n",
    "    'k_proj',\n",
    "    'v_proj',\n",
    "    'o_proj',\n",
    "    'gate_proj',\n",
    "    'up_proj',\n",
    "    'down_proj',\n",
    "  ],\n",
    "  bias='none',\n",
    "  lora_dropout=0.05,\n",
    "  task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from transformers import (DataCollatorForLanguageModeling,\n",
    "                          EarlyStoppingCallback, Trainer, TrainingArguments)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    args=TrainingArguments(\n",
    "        output_dir='./output',\n",
    "        logging_dir='./logs',\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=1e-3,\n",
    "        bf16=True,\n",
    "        optim='paged_adamw_8bit',\n",
    "        logging_strategy='steps',\n",
    "        logging_steps=200,\n",
    "        save_strategy='epoch',\n",
    "        save_steps=1,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=500,\n",
    "        do_eval=True,\n",
    "        # load_best_model_at_end=True,\n",
    "        # metric_for_best_model='loss',\n",
    "        # greater_is_better=False,\n",
    "        report_to='wandb',\n",
    "        run_name=run_name if run_name else\n",
    "          f'{project_name}-{datetime.now().strftime(\"%Y-%m-%d-%H-%M\")}'\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "model.config.use_cache = True\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(hub_model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
