{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Google's FLAN-T5 model for Topic Labeling\n",
    "\n",
    "by Andreas SÃ¼nder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages (only once)\n",
    "\n",
    "```bash\n",
    "%pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Open up a terminal and run the following commands:\n",
    "\n",
    "```bash\n",
    "huggingface-cli login\n",
    "wandb login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the following variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'textminr/topic-labeling'\n",
    "MODEL_NAME = 'google/flan-t5-xl'\n",
    "\n",
    "PROJECT_NAME = 'tl_qlora_flan-t5-xl'\n",
    "%env WANDB_PROJECT=$PROJECT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "dataset = dataset.rename_column('label', 'topic_label')\n",
    "\n",
    "print(f\"Train dataset size: {dataset['train'].num_rows}\")\n",
    "# print(f\"Test dataset size: {dataset['validation'].num_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = 'Provide a topic label: {}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "  load_in_4bit=True,\n",
    "  bnb_4bit_use_double_quant=True,\n",
    "  bnb_4bit_quant_type='nf4',\n",
    "  bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, torch_dtype=torch.bfloat16, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 130\n",
    "max_target_length = 30\n",
    "\n",
    "def preprocess_data(sample, padding: str = 'max_length'):\n",
    "  model_inputs = tokenizer(\n",
    "    [prompt_template.format(top_terms) for top_terms in sample['top_terms']],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_source_length\n",
    "  )\n",
    "\n",
    "  labels = tokenizer(\n",
    "    text_target=[label for label in sample['topic_label']],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_target_length\n",
    "  )\n",
    "\n",
    "  if padding == 'max_length':\n",
    "    labels['input_ids'] = [\n",
    "      [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels['input_ids']\n",
    "    ]\n",
    "\n",
    "  model_inputs['labels'] = labels['input_ids']\n",
    "  return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "  r=4,\n",
    "  lora_alpha=16,\n",
    "  target_modules=['q', 'k', 'v', 'o'],\n",
    "  bias='none',\n",
    "  lora_dropout=0.05,\n",
    "  task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "label_pad_token_id = -100\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "  tokenizer,\n",
    "  model=model,\n",
    "  label_pad_token_id=label_pad_token_id,\n",
    "  pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datetime import datetime\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "  output_dir=f'models/{PROJECT_NAME}',\n",
    "  per_device_train_batch_size=8,\n",
    "  per_device_eval_batch_size=8,\n",
    "  predict_with_generate=True,\n",
    "  optim='paged_adamw_8bit',\n",
    "  bf16=True,\n",
    "  num_train_epochs=1,\n",
    "  learning_rate=1e-3,\n",
    "  logging_steps=10,\n",
    "  logging_dir='./logs',\n",
    "  save_strategy='no',\n",
    "  # do_eval=True,\n",
    "  # evaluation_strategy='steps',\n",
    "  # eval_steps=200,\n",
    "  report_to='wandb',\n",
    "  run_name=f'{PROJECT_NAME}-{datetime.now().strftime(\"%Y-%m-%d-%H-%M\")}'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=tokenized_dataset['train'],\n",
    "  # eval_dataset=tokenized_dataset['validation'],\n",
    "  data_collator=data_collator,\n",
    "  tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub('textminr/tl-flan-t5-xl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
