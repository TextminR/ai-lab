torchrun --standalone --nnodes=1 --nproc_per_node=1 scripts/train_lora.py \
    --model_name_or_path mistralai/Mistral-7B-Instruct-v0.1 \
    --data_path textminr/squad_v2 \
    --add_eos_token True \
    --add_bos_token True \
    --max_seq_length 256 \
    --padding_side "left" \
    --lora_r 8 \
    --lora_alpha 16 \
    --lora_dropout 0.05 \
    --q_lora True \
    --bf16 True \
    --output_dir output/mistral_squad \
    --logging_dir logs \
    --per_device_train_batch_size 16 \
    --per_device_eval_batch_size 16 \
    --gradient_checkpointing True \
    --gradient_accumulation_steps 1 \
    --warmup_steps 100 \
    --num_train_epochs 1 \
    --learning_rate 1e-4 \
    --optim paged_adamw_8bit \
    --logging_strategy steps \
    --logging_steps 25 \
    --save_strategy steps \
    --save_steps 1000 \
    --save_total_limit 2 \
    --do_eval True \
    --evaluation_strategy steps \
    --eval_steps 1000 \
    --load_best_model_at_end True \
    --metric_for_best_model loss \
    --greater_is_better False \
    --report_to wandb \
    --run_name ner_mistral_squad_test2 \
    --wandb_project ner_mistral_qlora \
    --wandb_enable_checkpointing True
