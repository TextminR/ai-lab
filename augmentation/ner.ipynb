{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting NER data\n",
    "\n",
    "by Benjamin Kissinger & Andreas Sünder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "SOURCE_FILE = 'dataset.jsonl'\n",
    "TARGET_FILE = 'ner_data_augmented.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gensim.downloader as api\n",
    "from datasets import load_dataset\n",
    "\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce0574c807749769fb0ef0ee914d873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77530a3b79224ec192c0b960ef03d6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80abdc330bbc403986d899aaf3dd8610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('json', data_files=os.path.join(DATA_DIR, SOURCE_FILE), split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "swords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weighted_number(start_year: int = 1700, end_year: int = 2023):\n",
    "  import numpy as np\n",
    "\n",
    "  years = np.arange(start_year, end_year + 1)\n",
    "  #weights = np.linspace(1, 10, len(years))\n",
    "  #weights /= weights.sum()\n",
    "\n",
    "  return years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 311/311 [01:09<00:00,  4.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from langdetect import detect as lang_detect\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def base_augmentation(df):\n",
    "  special_words = ['da']\n",
    "\n",
    "  new_df = pd.DataFrame(columns=['prompt', 'response'])\n",
    "  vocab = model.key_to_index\n",
    "\n",
    "  for i in tqdm(range(len(df))):\n",
    "    row = df.iloc[i]\n",
    "\n",
    "    # TODO: add german language support\n",
    "    if lang_detect(row['prompt']) != 'en':\n",
    "      continue\n",
    "\n",
    "    prompt_tokenized = word_tokenize(row['prompt'])\n",
    "    prompt_tagged = pos_tag(prompt_tokenized)\n",
    "    prompt_cleaned = [word for word in prompt_tagged if word[1] in ('NN', 'NNS') and word[0] not in special_words]\n",
    "\n",
    "    words_to_replace = []\n",
    "    replace_list = []\n",
    "\n",
    "    for word in prompt_cleaned:\n",
    "      if word[0] not in vocab:\n",
    "        continue\n",
    "\n",
    "      ms = [word[0] for word in model.most_similar(word[0], topn=10)]\n",
    "      ms_tagged = pos_tag(ms)\n",
    "      ms_new = [\n",
    "        replacement[0] for replacement in ms_tagged \n",
    "        if replacement[1] == word[1] and\n",
    "        '_' not in replacement[0] and\n",
    "        word[0].lower() != replacement[0].lower() and\n",
    "        model.distance(word[0], replacement[0]) < 0.5\n",
    "      ]\n",
    "\n",
    "      words_to_replace.append(word[0])\n",
    "      ms_new.append(word[0])\n",
    "  \n",
    "      if len(ms_new) > 0:\n",
    "        replace_list.append(ms_new)\n",
    "\n",
    "    prompt_removed = row['prompt']\n",
    "    for word in words_to_replace:\n",
    "      prompt_removed = prompt_removed.replace(f\" {word} \", ' {} ')\n",
    "\n",
    "    # MAXIMUM OF REPLACEMENTS FOR EACH ENTRY\n",
    "    max_replacements = 2048\n",
    "    replace_combinations = list(itertools.product(*replace_list))\n",
    "    if len(replace_combinations) > max_replacements:\n",
    "     replace_combinations = random.sample(replace_combinations, max_replacements)\n",
    "\n",
    "    for combination in replace_combinations:\n",
    "      new_df = pd.concat([new_df, pd.DataFrame(\n",
    "          [[prompt_removed.format(*combination), row['response']]],\n",
    "          columns=['prompt', 'response']\n",
    "      )])\n",
    "\n",
    " \n",
    "  return new_df\n",
    "\n",
    "new_df = base_augmentation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27764\n"
     ]
    }
   ],
   "source": [
    "print(len(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2141/27764 [00:10<02:18, 185.04it/s]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "from faker import Faker\n",
    "from first import first\n",
    "\n",
    "MAX_NAME_COUNTER = 40\n",
    "\n",
    "def name_date_augmentation(df):\n",
    "  new_df = pd.DataFrame(columns=['prompt', 'response'])\n",
    "  author_replacements = {}\n",
    "  fake = Faker()\n",
    "\n",
    "  nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "  for i in tqdm(range(len(df))):\n",
    "    row = df.iloc[i]\n",
    "    prompt = row['prompt']\n",
    "    response = json.loads(row['response'])\n",
    "\n",
    "    doc = nlp(prompt)\n",
    "    author = first([ent.text for ent in doc.ents if ent.label_ == 'PERSON'])\n",
    "\n",
    "    if author:\n",
    "      prompt = prompt.replace(author, '{author}')\n",
    "\n",
    "      if author not in author_replacements:\n",
    "        author_replacements[author] = [fake.name(), 0]\n",
    "\n",
    "      counter = author_replacements[author][1]\n",
    "      if counter >= MAX_NAME_COUNTER:\n",
    "        counter = 0\n",
    "        author_replacements[author] = [fake.name(), 0]\n",
    "\n",
    "      author_new = author_replacements[author][0]\n",
    "      # print(author_replacements)\n",
    "      author_replacements[author][1] = counter + 1\n",
    "\n",
    "      # TODO: add name augmentation\n",
    "      prompt = prompt.format(author=author_new)\n",
    "      response['author'] = author_new\n",
    "\n",
    "    new_df = new_df.append({ 'prompt': prompt, 'response': json.dumps(response, default=str) }, ignore_index=True )\n",
    "    \n",
    "  return new_df\n",
    "\n",
    "final_df = name_date_augmentation(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               prompt  \\\n",
      "0   This texts was written by Robert Smith on the ...   \n",
      "0   This text was written by Robert Smith on the 4...   \n",
      "0   This is a memo from John Smith on 12th March 2...   \n",
      "0   This is a memorandum from John Smith on 12th M...   \n",
      "0   This is a statement from John Smith on 12th Ma...   \n",
      "..                                                ...   \n",
      "0   Mark Twain, in his work 'Adventures of Huckleb...   \n",
      "0   Mark Twain, in his work 'Adventures of Huckleb...   \n",
      "0   Mark Twain, in his work 'Adventures of Huckleb...   \n",
      "0   Mark Twain, in his work 'Adventures of Huckleb...   \n",
      "0   Mark Twain, in his work 'Adventures of Huckleb...   \n",
      "\n",
      "                                      response  \n",
      "0   {\"author\": \"Robert Smith\", \"date\": \"2020\"}  \n",
      "0   {\"author\": \"Robert Smith\", \"date\": \"2020\"}  \n",
      "0     {\"author\": \"John Smith\", \"date\": \"2018\"}  \n",
      "0     {\"author\": \"John Smith\", \"date\": \"2018\"}  \n",
      "0     {\"author\": \"John Smith\", \"date\": \"2018\"}  \n",
      "..                                         ...  \n",
      "0      {\"author\": \"Mark Twain\", \"date\": \"N/A\"}  \n",
      "0      {\"author\": \"Mark Twain\", \"date\": \"N/A\"}  \n",
      "0      {\"author\": \"Mark Twain\", \"date\": \"N/A\"}  \n",
      "0      {\"author\": \"Mark Twain\", \"date\": \"N/A\"}  \n",
      "0      {\"author\": \"Mark Twain\", \"date\": \"N/A\"}  \n",
      "\n",
      "[20127 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27764\n"
     ]
    }
   ],
   "source": [
    "print(len(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop_duplicates().sample(frac=1).reset_index(drop=True, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ben/ai-lab/augmentation/ner.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/ben/ai-lab/augmentation/ner.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(DATA_DIR, TARGET_FILE), \u001b[39m'\u001b[39m\u001b[39mw+\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/ben/ai-lab/augmentation/ner.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m   final_df\u001b[39m.\u001b[39;49mto_json(f, orient\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrecords\u001b[39m\u001b[39m'\u001b[39m, lines\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, force_ascii\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to_json'"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(DATA_DIR, TARGET_FILE), 'w+') as f:\n",
    "  final_df.to_json(f, orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20127\n"
     ]
    }
   ],
   "source": [
    "print(len(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
